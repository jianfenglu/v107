---
title: Gating creates slow modes and controls phase-space complexity in GRUs and LSTMs
abstract: 'Recurrent neural networks (RNNs) are powerful dynamical models for data
  with complex temporal structure. However, training RNNs has traditionally proved
  challenging due to exploding or vanishing of gradients. RNN models such as LSTMs
  and GRUs (and their variants) significantly mitigate these issues associated with
  training by introducing various types of {\it gating} units into the architecture.
  While these gates empirically improve performance, how the addition of gates influences
  the dynamics and trainability of GRUs and LSTMs is not well understood. Here, we
  take the perspective of studying randomly initialized LSTMs and GRUs as dynamical
  systems, and ask how the salient dynamical properties are shaped by the gates.  We
  leverage tools from random matrix theory and mean-field theory to study the state-to-state
  Jacobians of GRUs and LSTMs. We show that the update gate in the GRU and the forget
  gate in the LSTM can lead to an accumulation of slow modes in the dynamics. Moreover,
  the GRU update gate can poise the system at a marginally stable point. The reset
  gate in the GRU and the output and input gates in the LSTM control the spectral
  radius of the Jacobian, and the GRU reset gate also modulates the complexity of
  the landscape of fixed-points. Furthermore, for the GRU we obtain a phase diagram
  describing the statistical properties of fixed-points. We also provide a preliminary
  comparison of training performance to the various dynamical regimes realized by
  varying hyperparameters. Looking to the future, we have introduced a powerful set
  of techniques which can be adapted to a broad class of RNNs, to study the influence
  of various architectural choices on dynamics, and potentially motivate the principled
  discovery of novel architectures.  '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: can20a
month: 0
tex_title: "{Gating creates slow modes and controls phase-space complexity in GRUs
  and LSTMs}"
firstpage: 476
lastpage: 511
page: 476-511
order: 476
cycles: false
bibtex_author: Can, Tankut and Krishnamurthy, Kamesh and Schwab, David J.
author:
- given: Tankut
  family: Can
- given: Kamesh
  family: Krishnamurthy
- given: David J.
  family: Schwab
date: 2020-08-16
address: 
publisher: PMLR
container-title: Proceedings of The First Mathematical and Scientific Machine Learning
  Conference
volume: '107'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 16
pdf: http://proceedings.mlr.press/v107/can20a/can20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
