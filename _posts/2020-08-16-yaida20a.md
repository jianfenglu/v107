---
title: Non-Gaussian processes and neural networks at finite widths
abstract: 'Gaussian processes are ubiquitous in nature and engineering. A case in
  point is a class of neural networks in the infinite-width limit, whose priors correspond
  to Gaussian processes. Here we perturbatively extend this correspondence to finite-width
  neural networks, yielding non-Gaussian processes as priors. The methodology developed
  herein allows us to track the flow of preactivation distributions by progressively
  integrating out random variables from lower to higher layers, reminiscent of renormalization-group
  flow. We further develop a perturbative procedure to perform Bayesian inference
  with weakly non-Gaussian priors. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yaida20a
month: 0
tex_title: Non-{G}aussian processes and neural networks at finite widths
firstpage: 165
lastpage: 192
page: 165-192
order: 165
cycles: false
bibtex_author: Yaida, Sho
author:
- given: Sho
  family: Yaida
date: 2020-08-16
address: 
publisher: PMLR
container-title: Proceedings of The First Mathematical and Scientific Machine Learning
  Conference
volume: '107'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 16
pdf: http://proceedings.mlr.press/v107/yaida20a/yaida20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
