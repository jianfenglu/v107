---
title: Large deviations for the perceptron model and consequences for active learning
abstract: " Active learning is a branch of machine learning that deals with problems
  where unlabeled data is abundant yet obtaining labels is expensive. The learning
  algorithm has the possibility of querying a limited number of samples to obtain
  the corresponding labels, subsequently used for supervised learning. In this work,
  we consider the task of choosing the subset of samples to be labeled from a fixed
  finite pool of samples.  We assume the pool of samples to be a random matrix and
  the ground truth labels to be generated by a single-layer teacher random neural
  network. We employ replica methods to analyze the large deviations for the accuracy
  achieved after supervised learning on a subset of the original pool. These large
  deviations then provide optimal achievable performance boundaries for any active
  learning algorithm. We show that the optimal learning performance can be efficiently
  approached by simple message-passing active learning algorithms. We also provide
  a comparison with the performance of some other popular active learning strategies.
  \ "
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cui20a
month: 0
tex_title: Large deviations for the perceptron model and consequences for active learning
firstpage: 390
lastpage: 430
page: 390-430
order: 390
cycles: false
bibtex_author: Cui, Hugo and Saglietti, Luca and Zdeborova, Lenka
author:
- given: Hugo
  family: Cui
- given: Luca
  family: Saglietti
- given: Lenka
  family: Zdeborova
date: 2020-08-16
address: 
publisher: PMLR
container-title: Proceedings of The First Mathematical and Scientific Machine Learning
  Conference
volume: '107'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 16
pdf: http://proceedings.mlr.press/v107/cui20a/cui20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
